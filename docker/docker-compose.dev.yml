# See:
# - https://ram.app/docs/developer/setup
# - https://ram.app/docs/developer/troubleshooting

name: ram-dev

x-server-build: &server-common
  image: ram-server-dev:latest
  build:
    context: ../
    dockerfile: server/Dockerfile
    target: dev
  restart: always
  volumes:
    - ../server:/usr/src/app
    - ../open-api:/usr/src/open-api
    - ${UPLOAD_LOCATION}/photos:/usr/src/app/upload
    - ${UPLOAD_LOCATION}/photos/upload:/usr/src/app/upload/upload
    - /usr/src/app/node_modules
    - /etc/localtime:/etc/localtime:ro
  env_file:
    - .env
  ulimits:
    nofile:
      soft: 1048576
      hard: 1048576

services:
  ram-server:
    container_name: ram_server
    command: ['/usr/src/app/bin/ram-dev', 'ram']
    <<: *server-common
    ports:
      - 3001:3001
      - 9230:9230
    depends_on:
      - redis
      - database

  ram-microservices:
    container_name: ram_microservices
    command: ['/usr/src/app/bin/ram-dev', 'microservices']
    <<: *server-common
    # extends:
    #   file: hwaccel.transcoding.yml
    #   service: cpu # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding
    ports:
      - 9231:9230
    depends_on:
      - database
      - ram-server

  ram-web:
    container_name: ram_web
    image: ram-web-dev:latest
    build:
      context: ../web
    command: ['/usr/src/app/bin/ram-web']
    env_file:
      - .env
    ports:
      - 2283:3000
      - 24678:24678
    volumes:
      - ../web:/usr/src/app
      - ../open-api/:/usr/src/open-api/
      - /usr/src/app/node_modules
    ulimits:
      nofile:
        soft: 1048576
        hard: 1048576
    restart: unless-stopped
    depends_on:
      - ram-server

  ram-machine-learning:
    container_name: ram_machine_learning
    image: ram-machine-learning-dev:latest
    # extends:
    #   file: hwaccel.ml.yml
    #   service: cpu # set to one of [armnn, cuda, openvino, openvino-wsl] for accelerated inference
    build:
      context: ../machine-learning
      dockerfile: Dockerfile
      args:
        - DEVICE=cpu # set to one of [armnn, cuda, openvino, openvino-wsl] for accelerated inference
    ports:
      - 3003:3003
    volumes:
      - ../machine-learning:/usr/src/app
      - model-cache:/cache
    env_file:
      - .env
    depends_on:
      - database
    restart: unless-stopped

  redis:
    container_name: ram_redis
    image: redis:6.2-alpine@sha256:84882e87b54734154586e5f8abd4dce69fe7311315e2fc6d67c29614c8de2672

  database:
    container_name: ram_postgres
    image: tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:90724186f0a3517cf6914295b5ab410db9ce23190a2d9d0b9dd6463e3fa298f0
    env_file:
      - .env
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: ${DB_USERNAME}
      POSTGRES_DB: ${DB_DATABASE_NAME}
      POSTGRES_INITDB_ARGS: '--data-checksums'
    volumes:
      - ${UPLOAD_LOCATION}/postgres:/var/lib/postgresql/data
    ports:
      - 5432:5432
    command: ["postgres", "-c" ,"shared_preload_libraries=vectors.so", "-c", 'search_path="$$user", public, vectors', "-c", "logging_collector=on", "-c", "max_wal_size=2GB", "-c", "shared_buffers=512MB", "-c", "wal_compression=on"]

  # set ram_METRICS=true in .env to enable metrics
  # ram-prometheus:
  #   container_name: ram_prometheus
  #   ports:
  #     - 9090:9090
  #   image: prom/prometheus
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml
  #     - prometheus-data:/prometheus

  # first login uses admin/admin
  # add data source for http://ram-prometheus:9090 to get started
  # ram-grafana:
  #   container_name: ram_grafana
  #   command: ['./run.sh', '-disable-reporting']
  #   ports:
  #     - 3000:3000
  #   image: grafana/grafana:10.3.3-ubuntu
  #   volumes:
  #     - grafana-data:/var/lib/grafana

volumes:
  model-cache:
  prometheus-data:
  grafana-data:
